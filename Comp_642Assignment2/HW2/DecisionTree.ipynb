{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef1d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from pprint import pprint\n",
    "from decision_tree_functions import *\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd2526b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ccc8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Train Decision Train on Iris Data\n",
    "\n",
    "df = pd.read_csv(\"data/Iris.csv\")\n",
    "df = df.drop(\"Id\", axis=1)\n",
    "df = df.rename(columns={\"species\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60fdf9e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width        label\n",
       "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
       "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
       "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
       "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
       "4           5.0          3.6           1.4          0.2  Iris-setosa"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18402f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "train_data, test_data = train_test_split(df, test_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c35884a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [4.9, 3.0, 1.4, 0.2, 'Iris-setosa'],\n",
       "       [4.7, 3.2, 1.3, 0.2, 'Iris-setosa'],\n",
       "       [4.6, 3.1, 1.5, 0.2, 'Iris-setosa'],\n",
       "       [5.0, 3.6, 1.4, 0.2, 'Iris-setosa']], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_data.values\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefb58a",
   "metadata": {},
   "source": [
    "Note: Example Representation of Decision Tree:  \n",
    "example_tree = {\"petal_width <= 0.8\": [\"Iris-setosa\", \n",
    "                                      {\"petal_width <= 1.65\": [{\"petal_length <= 4.9\": [\"Iris-versicolor\", \n",
    "                                                                                        \"Iris-virginica\"]}, \n",
    "                                                                \"Iris-virginica\"]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b64abe0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'petal_width <= 0.6': ['Iris-setosa',\n",
      "                        {'petal_width <= 1.6': [{'petal_length <= 4.9': ['Iris-versicolor',\n",
      "                                                                         'Iris-virginica']},\n",
      "                                                'Iris-virginica']}]}\n"
     ]
    }
   ],
   "source": [
    "# Run the Decision Tree classifier on Training data\n",
    "\n",
    "tree = decision_tree_algorithm(train_data, max_depth=3)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deb7af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Iris data is: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Test Accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(test_data, tree)\n",
    "print (\"Test accuracy for Iris data is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a2280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Train Decision Tree on Titanic data\n",
    "\n",
    "df = pd.read_csv(\"data/Titanic.csv\")\n",
    "df[\"label\"] = df.Survived\n",
    "df = df.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# handling missing values\n",
    "median_age = df.Age.median()\n",
    "mode_embarked = df.Embarked.mode()[0]\n",
    "\n",
    "df = df.fillna({\"Age\": median_age, \"Embarked\": mode_embarked})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8991f1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for Titanic data is: 0.7808988764044944\n"
     ]
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "train_data, test_data = train_test_split(df, 0.2)\n",
    "tree = decision_tree_algorithm(train_data, max_depth=10)\n",
    "accuracy = calculate_accuracy(test_data, tree)\n",
    "\n",
    "accuracy = calculate_accuracy(test_data, tree)\n",
    "print (\"Test accuracy for Titanic data is: \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a3d536f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Prune Tree\n",
    "\n",
    "'''\n",
    "This function split the \"data\" into two parts according to the \"criterion\",\n",
    "where \"criterion\" consists of feature_name, comparison_operator, and the value for splitting,\n",
    "and \"data\" is a dataframe object that contains all the data points at this given node of the tree.\n",
    "'''\n",
    "def split_data(data, criterion):\n",
    "    feature, comparison_operator, value = criterion.split()\n",
    "    \n",
    "    # continuous feature\n",
    "    if comparison_operator == \"<=\":\n",
    "        left_data = data[data[feature] <= float(value)]\n",
    "        right_data = data[data[feature] >  float(value)]\n",
    "        \n",
    "    # categorical feature\n",
    "    else:\n",
    "        left_data = data[data[feature].astype(str) == value]\n",
    "        right_data  = data[data[feature].astype(str) != value]\n",
    "    \n",
    "    # Return the two partitions of the data after splitting\n",
    "    return left_data, right_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b971c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function returns the majority label value of data (if classification) or mean label value of data (if regression)\n",
    "'''\n",
    "def get_majority_or_mean_label(data, ml_task=\"classification\"):\n",
    "    # If task is regression, return the mean value of all the labels\n",
    "    if ml_task == \"regression\":\n",
    "        return data.label.mean()\n",
    "    \n",
    "    # If classification, return the majority class label\n",
    "    else:\n",
    "        return data.label.value_counts().index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa66143",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes data and a tree object (string if the tree is leaf, or dict object if it is not leaf),\n",
    "makes prediction on the data using the given tree and calculates the error\n",
    "Input:\n",
    "    node: a dataframe object that include all the data points in that given node of the tree\n",
    "    tree: a string/float value (if the tree is a leaf) or a dictionary object(if it is not a leaf) built recursively \n",
    "         in the following format {criterion : [left_subTree, right_subTree]}, where left_subTree and \n",
    "         right_subTree can be either a string/float (if it is leaf) or a dictionary (if it is not a leaf).\n",
    "         To see an example of what the tree object looks like, take a look at the output of one of the cells above. \n",
    "'''\n",
    "def calculate_error(data, tree, ml_task=\"classification\"):\n",
    "    prediction_labels = make_predictions(data, tree)\n",
    "    actual_labels = data.label\n",
    "    \n",
    "    if ml_task == \"regression\":\n",
    "        # mean squared error\n",
    "        return ((prediction_labels - actual_labels) **2).mean()\n",
    "    else:\n",
    "        # number of errors\n",
    "        return sum(prediction_labels != actual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7732c0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This is a helper function that may be invoked in the prune_algorithm() to decide whether to prune the tree or not.\n",
    "This function takes a tree object, training_data, validation_data as the input.\n",
    "The intuition behind this function is that it checks whether predicting with the given tree produces a better outcome than simply \n",
    "getting the majority/mean label from the training data. The way we check this is to use a validation dataset. We first \n",
    "compute the error of making prediction on the validation data using just the majority/mean label of the training data. \n",
    "We then compute the error of making prediction on the validation data using the tree. If the former is better \n",
    "than the latter, we discard the tree and simply return the majority class label (i.e. we prune the tree). Otherwise, \n",
    "we keep the original tree and return it.\n",
    "'''\n",
    "def prune_tree_helper(tree, data_train, data_val, ml_task=\"classification\"):\n",
    "    \n",
    "    majority_label = get_majority_or_mean_label(data_train, ml_task)\n",
    "    errors_using_majority_label = calculate_error(data_val, majority_label, ml_task)\n",
    "    \n",
    "    errors_using_tree = calculate_error(data_val, tree, ml_task)\n",
    "\n",
    "    if errors_using_majority_label <= errors_using_tree:\n",
    "        return majority_label\n",
    "    else:\n",
    "        return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f7329",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO: Complete the implementation of this function\n",
    "\n",
    "This function takes a given tree, training data, and validation data, and prune the tree based on the \n",
    "prediction performance on the validation data at each node of the tree. This function will be implemented\n",
    "recursively.\n",
    "\n",
    "Input: \n",
    "    cur_tree: a dict object (if the tree is not a leaf), or a string/float value (if the tree is a leaf)\n",
    "    data_train: a dataframe object represents the training data at this given node of the tree\n",
    "    data_val: a dataframe object represents the validation data at this given node of the tree\n",
    "    ml_task: a string that is either \"regression\" or \"classification\"\n",
    "Return:\n",
    "    a tree that is pruned (a string/float value if the tree is a single leaf, or a dict object if\n",
    "    the tree has branches)\n",
    "'''\n",
    "\n",
    "def prune_algorithm(cur_tree, data_train, data_val, ml_task=\"classification\"):\n",
    "    \n",
    "    if not isinstance(cur_tree, dict):\n",
    "        ################################################################\n",
    "        # Base case: if tree is a single leaf, just return the tree\n",
    "        # TODO: ~1 line of code expected\n",
    "        #\n",
    "        ################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:    \n",
    "        criterion = list(cur_tree.keys())[0]\n",
    "        left_tree, right_tree = cur_tree[criterion]\n",
    "        \n",
    "        ################################################################\n",
    "        # Recursive case: if tree has branches, do the following steps\n",
    "        # 1. split data_train into two parts \"data_train_left\" and \"data_train_right\" based on the \"criterion\" \n",
    "        #    (hint: use split_data method)\n",
    "        # 2. split data_val into two parts \"data_val_left\" and \"data_val_right\" based on the \"criterion\" \n",
    "        #    (hint: use split_data method)\n",
    "        # 3. recursively call prune_algorithm on left_tree, data_train_left, data_val_left to get the pruned left tree\n",
    "        # 4. recursively call prune_algorithm on right_tree, data_train_right, data_val_right to get the pruned right tree\n",
    "        # 5. Update the cur_tree as cur_tree = {criterion: [pruned_left_tree, pruned_right_tree]}\n",
    "        # 6. Prune the cur_tree that is updated in step 5 and return the pruned tree (if the tree doesn't need to be \n",
    "        #    pruned, just return cur_tree.(hint: you may use prune_tree_helper to decide whether to prune a tree or not)\n",
    "        # TODO: ~6 line of code expected\n",
    "        #\n",
    "        ################################################################\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "After you implemented prune_algorithm function, run the following cells to compare the test\n",
    "accuracy of pre-pruned tree and post-pruned tree on TiTanic Dataset. You will get a plot that \n",
    "compares the two. No modification of code is needed.\n",
    "'''\n",
    "\n",
    "# Load Titanic Data\n",
    "\n",
    "df = pd.read_csv(\"data/Titanic.csv\")\n",
    "df[\"label\"] = df.Survived\n",
    "df = df.drop([\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\"], axis=1)\n",
    "\n",
    "# handling missing values\n",
    "median_age = df.Age.median()\n",
    "mode_embarked = df.Embarked.mode()[0]\n",
    "\n",
    "df = df.fillna({\"Age\": median_age, \"Embarked\": mode_embarked})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fefb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare test accuracy of Decision Tree before and after pruning\n",
    "random.seed(1)\n",
    "metrics = {\"max_depth\": [], \"acc_tree\": [], \"acc_tree_pruned\": []}\n",
    "for n in range(10, 26):\n",
    "    df_train, df_test = train_test_split(df, test_size=0.15)\n",
    "    df_train, df_val = train_test_split(df_train, test_size=0.15)\n",
    "\n",
    "    tree = decision_tree_algorithm(df_train, ml_task=\"classification\", max_depth=n)\n",
    "    tree_pruned = prune_algorithm(tree, df_train, df_val, ml_task=\"classification\")\n",
    "    \n",
    "    metrics[\"max_depth\"].append(n)\n",
    "    metrics[\"acc_tree\"].append(calculate_accuracy(df_test, tree))\n",
    "    metrics[\"acc_tree_pruned\"].append(calculate_accuracy(df_test, tree_pruned))\n",
    "    \n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics = df_metrics.set_index(\"max_depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e08dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "df_metrics.plot(figsize=(12, 5), marker=\"o\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
